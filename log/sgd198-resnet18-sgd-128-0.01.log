2022-02-09 20:59:29,493 [main.py] => lr:0.01
 epochs:2
 batch_size:128
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 test_root:resnet18

2022-02-09 21:00:10,645 [main.py] => lr:0.01
 epochs:2
 batch_size:128
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 test_root:resnet18

2022-02-09 21:01:10,928 [main.py] => lr:0.01
 epochs:2
 batch_size:128
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 test_root:resnet18

2022-02-09 21:04:00,392 [main.py] => lr:0.01
 epochs:2
 batch_size:128
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 net_type:resnet18

2022-02-09 21:05:00,197 [main.py] => lr:0.01
 epochs:2
 batch_size:128
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 net_type:resnet18

2022-02-09 21:05:35,635 [main.py] => Epoch 1/2 => Loss 4.961, Train_accy 1.70, Test_accy 2.01
2022-02-09 21:05:52,524 [main.py] => Epoch 2/2 => Loss 4.725, Train_accy 4.29, Test_accy 5.80
2022-02-09 21:07:20,848 [main.py] => lr:0.01
 epochs:200
 batch_size:128
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 net_type:resnet18

2022-02-09 21:07:54,388 [main.py] => Epoch 1/200 => Loss 5.001, Train_accy 2.53, Test_accy 1.78
2022-02-09 21:08:11,135 [main.py] => Epoch 2/200 => Loss 4.715, Train_accy 4.53, Test_accy 6.24
2022-02-09 21:08:28,711 [main.py] => Epoch 3/200 => Loss 4.480, Train_accy 6.88, Test_accy 7.69
2022-02-09 21:08:44,953 [main.py] => Epoch 4/200 => Loss 4.274, Train_accy 8.49, Test_accy 6.35
2022-02-09 21:09:01,769 [main.py] => Epoch 5/200 => Loss 4.112, Train_accy 9.86, Test_accy 8.92
2022-02-09 21:09:18,791 [main.py] => Epoch 6/200 => Loss 3.981, Train_accy 11.44, Test_accy 7.47
2022-02-09 21:09:35,860 [main.py] => Epoch 7/200 => Loss 3.869, Train_accy 13.60, Test_accy 8.81
2022-02-09 21:09:53,312 [main.py] => Epoch 8/200 => Loss 3.745, Train_accy 14.54, Test_accy 10.81
2022-02-09 21:10:09,733 [main.py] => Epoch 9/200 => Loss 3.621, Train_accy 16.34, Test_accy 10.37
2022-02-09 21:10:26,633 [main.py] => Epoch 10/200 => Loss 3.513, Train_accy 18.19, Test_accy 12.93
2022-02-09 21:10:43,962 [main.py] => Epoch 11/200 => Loss 3.447, Train_accy 19.04, Test_accy 14.94
2022-02-09 21:11:01,011 [main.py] => Epoch 12/200 => Loss 3.352, Train_accy 20.90, Test_accy 10.37
2022-02-09 21:11:17,800 [main.py] => Epoch 13/200 => Loss 3.253, Train_accy 22.42, Test_accy 15.05
2022-02-09 21:11:34,323 [main.py] => Epoch 14/200 => Loss 3.192, Train_accy 22.97, Test_accy 14.49
2022-02-09 21:11:50,518 [main.py] => Epoch 15/200 => Loss 3.125, Train_accy 23.76, Test_accy 14.16
2022-02-09 21:12:07,777 [main.py] => Epoch 16/200 => Loss 3.091, Train_accy 25.37, Test_accy 15.27
2022-02-09 21:12:24,893 [main.py] => Epoch 17/200 => Loss 2.977, Train_accy 27.44, Test_accy 17.39
2022-02-09 21:12:41,374 [main.py] => Epoch 18/200 => Loss 2.879, Train_accy 28.29, Test_accy 20.40
2022-02-09 21:12:58,848 [main.py] => Epoch 19/200 => Loss 2.845, Train_accy 28.63, Test_accy 18.62
2022-02-09 21:13:16,410 [main.py] => Epoch 20/200 => Loss 2.755, Train_accy 30.12, Test_accy 18.95
2022-02-09 21:13:33,814 [main.py] => Epoch 21/200 => Loss 2.695, Train_accy 31.34, Test_accy 20.62
2022-02-09 21:13:50,902 [main.py] => Epoch 22/200 => Loss 2.612, Train_accy 34.16, Test_accy 19.73
2022-02-09 21:14:08,266 [main.py] => Epoch 23/200 => Loss 2.571, Train_accy 33.56, Test_accy 20.51
2022-02-09 21:14:25,626 [main.py] => Epoch 24/200 => Loss 2.539, Train_accy 35.44, Test_accy 22.63
2022-02-09 21:14:42,769 [main.py] => Epoch 25/200 => Loss 2.463, Train_accy 36.60, Test_accy 24.64
2022-02-09 21:14:59,825 [main.py] => Epoch 26/200 => Loss 2.368, Train_accy 38.00, Test_accy 23.52
2022-02-09 21:15:17,358 [main.py] => Epoch 27/200 => Loss 2.361, Train_accy 39.25, Test_accy 24.41
2022-02-09 21:15:34,189 [main.py] => Epoch 28/200 => Loss 2.326, Train_accy 39.55, Test_accy 24.75
2022-02-09 21:15:52,205 [main.py] => Epoch 29/200 => Loss 2.269, Train_accy 40.86, Test_accy 16.16
2022-02-09 21:16:09,196 [main.py] => Epoch 30/200 => Loss 2.198, Train_accy 42.04, Test_accy 22.30
2022-02-09 21:16:25,707 [main.py] => Epoch 31/200 => Loss 2.107, Train_accy 44.17, Test_accy 25.53
2022-02-09 21:16:43,150 [main.py] => Epoch 32/200 => Loss 2.080, Train_accy 45.70, Test_accy 25.53
2022-02-09 21:17:00,955 [main.py] => Epoch 33/200 => Loss 2.050, Train_accy 46.18, Test_accy 23.41
2022-02-09 21:17:17,145 [main.py] => Epoch 34/200 => Loss 1.970, Train_accy 47.61, Test_accy 25.75
2022-02-09 21:17:34,153 [main.py] => Epoch 35/200 => Loss 1.919, Train_accy 49.29, Test_accy 26.98
2022-02-09 21:17:51,259 [main.py] => Epoch 36/200 => Loss 1.861, Train_accy 50.14, Test_accy 26.20
2022-02-09 21:18:08,210 [main.py] => Epoch 37/200 => Loss 1.827, Train_accy 50.71, Test_accy 25.98
2022-02-09 21:18:24,897 [main.py] => Epoch 38/200 => Loss 1.778, Train_accy 51.45, Test_accy 27.87
2022-02-09 21:18:41,227 [main.py] => Epoch 39/200 => Loss 1.733, Train_accy 53.00, Test_accy 27.42
2022-02-09 21:18:58,668 [main.py] => Epoch 40/200 => Loss 1.705, Train_accy 53.51, Test_accy 26.98
2022-02-09 21:19:15,377 [main.py] => Epoch 41/200 => Loss 1.651, Train_accy 55.89, Test_accy 26.42
2022-02-09 21:19:32,271 [main.py] => Epoch 42/200 => Loss 1.618, Train_accy 56.01, Test_accy 29.77
2022-02-09 21:19:49,864 [main.py] => Epoch 43/200 => Loss 1.484, Train_accy 60.85, Test_accy 30.77
2022-02-09 21:20:06,556 [main.py] => Epoch 44/200 => Loss 1.457, Train_accy 60.66, Test_accy 33.78
2022-02-09 21:20:23,222 [main.py] => Epoch 45/200 => Loss 1.413, Train_accy 61.03, Test_accy 30.32
2022-02-09 21:20:39,782 [main.py] => Epoch 46/200 => Loss 1.348, Train_accy 64.13, Test_accy 31.55
2022-02-09 21:20:55,782 [main.py] => Epoch 47/200 => Loss 1.319, Train_accy 64.13, Test_accy 33.67
2022-02-09 21:21:12,294 [main.py] => Epoch 48/200 => Loss 1.286, Train_accy 64.68, Test_accy 30.43
2022-02-09 21:21:29,236 [main.py] => Epoch 49/200 => Loss 1.275, Train_accy 65.01, Test_accy 28.43
2022-02-09 21:21:46,547 [main.py] => Epoch 50/200 => Loss 1.219, Train_accy 66.96, Test_accy 33.00
2022-02-09 21:22:03,159 [main.py] => Epoch 51/200 => Loss 1.183, Train_accy 66.75, Test_accy 33.33
2022-02-09 21:22:19,688 [main.py] => Epoch 52/200 => Loss 1.144, Train_accy 69.39, Test_accy 32.22
2022-02-09 21:22:36,618 [main.py] => Epoch 53/200 => Loss 1.109, Train_accy 69.55, Test_accy 33.89
2022-02-09 21:22:53,615 [main.py] => Epoch 54/200 => Loss 1.071, Train_accy 70.55, Test_accy 33.33
2022-02-09 21:23:11,222 [main.py] => Epoch 55/200 => Loss 1.040, Train_accy 71.16, Test_accy 35.23
2022-02-09 21:23:28,039 [main.py] => Epoch 56/200 => Loss 0.985, Train_accy 72.92, Test_accy 34.34
2022-02-09 21:23:45,132 [main.py] => Epoch 57/200 => Loss 0.990, Train_accy 73.50, Test_accy 34.67
2022-02-09 21:24:02,414 [main.py] => Epoch 58/200 => Loss 0.904, Train_accy 75.24, Test_accy 32.22
2022-02-09 21:24:19,345 [main.py] => Epoch 59/200 => Loss 0.915, Train_accy 74.99, Test_accy 37.35
2022-02-09 21:24:38,030 [main.py] => Epoch 60/200 => Loss 0.897, Train_accy 75.94, Test_accy 35.79
2022-02-09 21:24:56,367 [main.py] => Epoch 61/200 => Loss 0.827, Train_accy 78.03, Test_accy 36.79
2022-02-09 21:25:13,554 [main.py] => Epoch 62/200 => Loss 0.775, Train_accy 78.95, Test_accy 34.23
2022-02-09 21:25:31,368 [main.py] => Epoch 63/200 => Loss 0.798, Train_accy 78.89, Test_accy 36.90
2022-02-09 21:25:49,025 [main.py] => Epoch 64/200 => Loss 0.780, Train_accy 78.73, Test_accy 35.23
2022-02-09 21:26:07,448 [main.py] => Epoch 65/200 => Loss 0.732, Train_accy 80.68, Test_accy 36.57
2022-02-09 21:26:25,723 [main.py] => Epoch 66/200 => Loss 0.706, Train_accy 81.62, Test_accy 39.80
2022-02-09 21:26:43,614 [main.py] => Epoch 67/200 => Loss 0.666, Train_accy 82.08, Test_accy 38.57
2022-02-09 21:27:00,239 [main.py] => Epoch 68/200 => Loss 0.643, Train_accy 83.42, Test_accy 33.89
2022-02-09 21:27:17,531 [main.py] => Epoch 69/200 => Loss 0.635, Train_accy 83.60, Test_accy 37.79
2022-02-09 21:27:35,227 [main.py] => Epoch 70/200 => Loss 0.615, Train_accy 83.42, Test_accy 37.24
2022-02-09 21:27:53,322 [main.py] => Epoch 71/200 => Loss 0.625, Train_accy 84.12, Test_accy 36.45
2022-02-09 21:28:10,999 [main.py] => Epoch 72/200 => Loss 0.544, Train_accy 85.52, Test_accy 40.80
2022-02-09 21:28:27,947 [main.py] => Epoch 73/200 => Loss 0.568, Train_accy 85.03, Test_accy 39.91
2022-02-09 21:28:44,646 [main.py] => Epoch 74/200 => Loss 0.535, Train_accy 85.91, Test_accy 39.35
2022-02-09 21:29:01,896 [main.py] => Epoch 75/200 => Loss 0.540, Train_accy 85.88, Test_accy 37.24
2022-02-09 21:29:18,623 [main.py] => Epoch 76/200 => Loss 0.509, Train_accy 86.40, Test_accy 40.02
2022-02-09 21:29:35,714 [main.py] => Epoch 77/200 => Loss 0.462, Train_accy 87.77, Test_accy 39.80
2022-02-09 21:29:54,821 [main.py] => Epoch 78/200 => Loss 0.463, Train_accy 88.47, Test_accy 41.47
2022-02-09 21:30:12,529 [main.py] => Epoch 79/200 => Loss 0.450, Train_accy 87.92, Test_accy 41.03
2022-02-09 21:30:30,413 [main.py] => Epoch 80/200 => Loss 0.469, Train_accy 87.31, Test_accy 39.02
2022-02-09 21:30:48,336 [main.py] => Epoch 81/200 => Loss 0.471, Train_accy 88.01, Test_accy 41.03
2022-02-09 21:31:06,051 [main.py] => Epoch 82/200 => Loss 0.430, Train_accy 88.44, Test_accy 39.46
2022-02-09 21:31:23,767 [main.py] => Epoch 83/200 => Loss 0.433, Train_accy 88.74, Test_accy 39.58
2022-02-09 21:31:40,561 [main.py] => Epoch 84/200 => Loss 0.438, Train_accy 88.83, Test_accy 42.47
2022-02-09 21:31:57,676 [main.py] => Epoch 85/200 => Loss 0.416, Train_accy 89.63, Test_accy 42.92
2022-02-09 21:32:15,119 [main.py] => Epoch 86/200 => Loss 0.379, Train_accy 90.69, Test_accy 42.25
2022-02-09 21:32:32,864 [main.py] => Epoch 87/200 => Loss 0.349, Train_accy 91.69, Test_accy 44.70
2022-02-09 21:32:50,152 [main.py] => Epoch 88/200 => Loss 0.347, Train_accy 91.09, Test_accy 43.70
2022-02-09 21:33:07,450 [main.py] => Epoch 89/200 => Loss 0.331, Train_accy 92.27, Test_accy 43.26
2022-02-09 21:33:25,799 [main.py] => Epoch 90/200 => Loss 0.364, Train_accy 90.20, Test_accy 38.91
2022-02-09 21:33:44,873 [main.py] => Epoch 91/200 => Loss 0.334, Train_accy 91.82, Test_accy 41.92
2022-02-09 21:34:01,941 [main.py] => Epoch 92/200 => Loss 0.319, Train_accy 91.48, Test_accy 40.47
2022-02-09 21:34:18,674 [main.py] => Epoch 93/200 => Loss 0.328, Train_accy 91.21, Test_accy 43.37
2022-02-09 21:34:37,210 [main.py] => Epoch 94/200 => Loss 0.325, Train_accy 91.45, Test_accy 42.92
2022-02-09 21:34:54,853 [main.py] => Epoch 95/200 => Loss 0.315, Train_accy 92.00, Test_accy 42.70
2022-02-09 21:35:11,405 [main.py] => Epoch 96/200 => Loss 0.321, Train_accy 92.27, Test_accy 41.47
2022-02-09 21:35:28,984 [main.py] => Epoch 97/200 => Loss 0.313, Train_accy 92.00, Test_accy 44.04
2022-02-09 21:35:46,345 [main.py] => Epoch 98/200 => Loss 0.294, Train_accy 92.18, Test_accy 42.36
2022-02-09 21:36:03,785 [main.py] => Epoch 99/200 => Loss 0.258, Train_accy 93.49, Test_accy 41.81
2022-02-09 21:36:23,122 [main.py] => Epoch 100/200 => Loss 0.258, Train_accy 93.76, Test_accy 43.14
2022-02-09 21:36:40,446 [main.py] => Epoch 101/200 => Loss 0.194, Train_accy 95.77, Test_accy 49.16
2022-02-09 21:36:57,827 [main.py] => Epoch 102/200 => Loss 0.163, Train_accy 96.50, Test_accy 50.39
2022-02-09 21:37:15,774 [main.py] => Epoch 103/200 => Loss 0.147, Train_accy 96.96, Test_accy 49.83
2022-02-09 21:37:33,187 [main.py] => Epoch 104/200 => Loss 0.144, Train_accy 97.35, Test_accy 50.06
2022-02-09 21:37:50,956 [main.py] => Epoch 105/200 => Loss 0.125, Train_accy 97.69, Test_accy 49.94
2022-02-09 21:38:09,730 [main.py] => Epoch 106/200 => Loss 0.124, Train_accy 97.87, Test_accy 49.94
2022-02-09 21:38:27,094 [main.py] => Epoch 107/200 => Loss 0.124, Train_accy 98.02, Test_accy 50.84
2022-02-09 21:38:44,253 [main.py] => Epoch 108/200 => Loss 0.120, Train_accy 97.99, Test_accy 50.06
2022-02-09 21:39:00,837 [main.py] => Epoch 109/200 => Loss 0.130, Train_accy 97.38, Test_accy 50.39
2022-02-09 21:39:17,136 [main.py] => Epoch 110/200 => Loss 0.121, Train_accy 98.02, Test_accy 50.39
2022-02-09 21:39:33,623 [main.py] => Epoch 111/200 => Loss 0.111, Train_accy 98.27, Test_accy 50.06
2022-02-09 21:39:49,959 [main.py] => Epoch 112/200 => Loss 0.115, Train_accy 98.30, Test_accy 50.61
2022-02-09 21:40:07,024 [main.py] => Epoch 113/200 => Loss 0.109, Train_accy 98.30, Test_accy 50.06
2022-02-09 21:40:23,179 [main.py] => Epoch 114/200 => Loss 0.114, Train_accy 98.05, Test_accy 51.28
2022-02-09 21:40:39,946 [main.py] => Epoch 115/200 => Loss 0.102, Train_accy 98.17, Test_accy 50.28
2022-02-09 21:40:57,092 [main.py] => Epoch 116/200 => Loss 0.110, Train_accy 98.08, Test_accy 50.95
2022-02-09 21:41:14,098 [main.py] => Epoch 117/200 => Loss 0.107, Train_accy 98.17, Test_accy 51.06
2022-02-09 21:41:31,137 [main.py] => Epoch 118/200 => Loss 0.105, Train_accy 98.33, Test_accy 50.39
2022-02-09 21:41:47,682 [main.py] => Epoch 119/200 => Loss 0.099, Train_accy 98.33, Test_accy 50.28
2022-02-09 21:42:04,055 [main.py] => Epoch 120/200 => Loss 0.108, Train_accy 98.17, Test_accy 49.72
2022-02-09 21:42:20,200 [main.py] => Epoch 121/200 => Loss 0.102, Train_accy 98.63, Test_accy 50.28
2022-02-09 21:42:37,162 [main.py] => Epoch 122/200 => Loss 0.110, Train_accy 98.08, Test_accy 49.61
2022-02-09 21:42:53,859 [main.py] => Epoch 123/200 => Loss 0.113, Train_accy 98.11, Test_accy 51.06
2022-02-09 21:43:11,332 [main.py] => Epoch 124/200 => Loss 0.112, Train_accy 98.02, Test_accy 50.17
2022-02-09 21:43:28,536 [main.py] => Epoch 125/200 => Loss 0.112, Train_accy 98.14, Test_accy 49.72
2022-02-09 21:43:45,163 [main.py] => Epoch 126/200 => Loss 0.102, Train_accy 98.45, Test_accy 50.28
2022-02-09 21:44:00,755 [main.py] => Epoch 127/200 => Loss 0.108, Train_accy 98.21, Test_accy 50.50
2022-02-09 21:44:17,987 [main.py] => Epoch 128/200 => Loss 0.098, Train_accy 98.27, Test_accy 50.50
2022-02-09 21:44:34,894 [main.py] => Epoch 129/200 => Loss 0.109, Train_accy 98.21, Test_accy 50.28
2022-02-09 21:44:51,234 [main.py] => Epoch 130/200 => Loss 0.093, Train_accy 98.75, Test_accy 50.95
2022-02-09 21:45:08,034 [main.py] => Epoch 131/200 => Loss 0.102, Train_accy 98.39, Test_accy 51.06
2022-02-09 21:45:24,297 [main.py] => Epoch 132/200 => Loss 0.089, Train_accy 98.72, Test_accy 50.72
2022-02-09 21:45:41,578 [main.py] => Epoch 133/200 => Loss 0.098, Train_accy 98.39, Test_accy 50.17
2022-02-09 21:45:57,652 [main.py] => Epoch 134/200 => Loss 0.085, Train_accy 98.66, Test_accy 49.83
2022-02-09 21:46:14,521 [main.py] => Epoch 135/200 => Loss 0.100, Train_accy 98.30, Test_accy 49.83
2022-02-09 21:46:31,241 [main.py] => Epoch 136/200 => Loss 0.092, Train_accy 98.66, Test_accy 50.06
2022-02-09 21:46:47,710 [main.py] => Epoch 137/200 => Loss 0.094, Train_accy 98.63, Test_accy 50.39
2022-02-09 21:47:04,684 [main.py] => Epoch 138/200 => Loss 0.094, Train_accy 98.60, Test_accy 50.95
2022-02-09 21:47:21,347 [main.py] => Epoch 139/200 => Loss 0.101, Train_accy 98.21, Test_accy 50.28
2022-02-09 21:47:39,007 [main.py] => Epoch 140/200 => Loss 0.090, Train_accy 98.66, Test_accy 49.83
2022-02-09 21:47:56,583 [main.py] => Epoch 141/200 => Loss 0.087, Train_accy 98.72, Test_accy 51.28
2022-02-09 21:48:14,542 [main.py] => Epoch 142/200 => Loss 0.111, Train_accy 98.05, Test_accy 50.72
2022-02-09 21:48:33,694 [main.py] => Epoch 143/200 => Loss 0.088, Train_accy 98.69, Test_accy 51.06
2022-02-09 21:48:50,912 [main.py] => Epoch 144/200 => Loss 0.094, Train_accy 98.84, Test_accy 50.39
2022-02-09 21:49:08,584 [main.py] => Epoch 145/200 => Loss 0.098, Train_accy 98.33, Test_accy 50.61
2022-02-09 21:49:27,948 [main.py] => Epoch 146/200 => Loss 0.098, Train_accy 98.27, Test_accy 50.17
2022-02-09 21:49:47,008 [main.py] => Epoch 147/200 => Loss 0.085, Train_accy 99.00, Test_accy 50.28
2022-02-09 21:50:05,969 [main.py] => Epoch 148/200 => Loss 0.090, Train_accy 98.51, Test_accy 50.28
2022-02-09 21:50:23,769 [main.py] => Epoch 149/200 => Loss 0.088, Train_accy 98.69, Test_accy 49.83
2022-02-09 21:50:42,103 [main.py] => Epoch 150/200 => Loss 0.096, Train_accy 98.27, Test_accy 51.06
2022-02-09 21:51:00,337 [main.py] => Epoch 151/200 => Loss 0.088, Train_accy 98.57, Test_accy 51.06
2022-02-09 21:51:18,056 [main.py] => Epoch 152/200 => Loss 0.087, Train_accy 98.51, Test_accy 50.84
2022-02-09 21:51:36,381 [main.py] => Epoch 153/200 => Loss 0.094, Train_accy 98.84, Test_accy 51.28
2022-02-09 21:51:55,531 [main.py] => Epoch 154/200 => Loss 0.090, Train_accy 98.69, Test_accy 51.17
2022-02-09 21:52:12,997 [main.py] => Epoch 155/200 => Loss 0.091, Train_accy 98.60, Test_accy 50.84
2022-02-09 21:52:30,960 [main.py] => Epoch 156/200 => Loss 0.081, Train_accy 98.72, Test_accy 50.95
2022-02-09 21:52:49,160 [main.py] => Epoch 157/200 => Loss 0.082, Train_accy 98.78, Test_accy 51.06
2022-02-09 21:53:07,208 [main.py] => Epoch 158/200 => Loss 0.087, Train_accy 98.63, Test_accy 51.06
2022-02-09 21:53:26,139 [main.py] => Epoch 159/200 => Loss 0.084, Train_accy 98.94, Test_accy 51.39
2022-02-09 21:53:43,575 [main.py] => Epoch 160/200 => Loss 0.089, Train_accy 98.57, Test_accy 51.17
2022-02-09 21:54:01,625 [main.py] => Epoch 161/200 => Loss 0.089, Train_accy 98.66, Test_accy 51.28
2022-02-09 21:54:20,358 [main.py] => Epoch 162/200 => Loss 0.087, Train_accy 98.63, Test_accy 51.28
2022-02-09 21:54:38,282 [main.py] => Epoch 163/200 => Loss 0.091, Train_accy 98.63, Test_accy 51.84
2022-02-09 21:54:56,889 [main.py] => Epoch 164/200 => Loss 0.089, Train_accy 98.60, Test_accy 50.84
2022-02-09 21:55:14,872 [main.py] => Epoch 165/200 => Loss 0.083, Train_accy 98.94, Test_accy 51.06
2022-02-09 21:55:33,675 [main.py] => Epoch 166/200 => Loss 0.086, Train_accy 98.84, Test_accy 51.17
2022-02-09 21:55:52,514 [main.py] => Epoch 167/200 => Loss 0.089, Train_accy 98.24, Test_accy 51.06
2022-02-09 21:56:10,402 [main.py] => Epoch 168/200 => Loss 0.094, Train_accy 98.42, Test_accy 51.06
2022-02-09 21:56:28,743 [main.py] => Epoch 169/200 => Loss 0.083, Train_accy 98.97, Test_accy 50.95
2022-02-09 21:56:47,584 [main.py] => Epoch 170/200 => Loss 0.081, Train_accy 99.18, Test_accy 51.28
2022-02-09 21:57:06,006 [main.py] => Epoch 171/200 => Loss 0.086, Train_accy 98.69, Test_accy 50.84
2022-02-09 21:57:24,097 [main.py] => Epoch 172/200 => Loss 0.090, Train_accy 98.69, Test_accy 50.50
2022-02-09 21:57:43,476 [main.py] => Epoch 173/200 => Loss 0.085, Train_accy 98.54, Test_accy 51.28
2022-02-09 21:58:02,945 [main.py] => Epoch 174/200 => Loss 0.086, Train_accy 98.72, Test_accy 50.95
2022-02-09 21:58:23,003 [main.py] => Epoch 175/200 => Loss 0.085, Train_accy 98.72, Test_accy 50.95
2022-02-09 21:58:43,315 [main.py] => Epoch 176/200 => Loss 0.081, Train_accy 98.78, Test_accy 50.95
2022-02-09 21:59:04,138 [main.py] => Epoch 177/200 => Loss 0.087, Train_accy 98.54, Test_accy 51.62
2022-02-09 21:59:25,622 [main.py] => Epoch 178/200 => Loss 0.086, Train_accy 98.66, Test_accy 50.84
2022-02-09 21:59:47,876 [main.py] => Epoch 179/200 => Loss 0.081, Train_accy 99.00, Test_accy 50.61
2022-02-09 22:00:11,433 [main.py] => Epoch 180/200 => Loss 0.084, Train_accy 98.81, Test_accy 50.50
2022-02-09 22:00:35,823 [main.py] => Epoch 181/200 => Loss 0.084, Train_accy 98.60, Test_accy 50.72
2022-02-09 22:00:59,224 [main.py] => Epoch 182/200 => Loss 0.088, Train_accy 98.54, Test_accy 51.51
2022-02-09 22:01:21,778 [main.py] => Epoch 183/200 => Loss 0.096, Train_accy 98.54, Test_accy 50.95
2022-02-09 22:01:43,983 [main.py] => Epoch 184/200 => Loss 0.091, Train_accy 98.63, Test_accy 50.84
2022-02-09 22:02:06,755 [main.py] => Epoch 185/200 => Loss 0.077, Train_accy 98.84, Test_accy 50.72
2022-02-09 22:02:28,839 [main.py] => Epoch 186/200 => Loss 0.085, Train_accy 98.97, Test_accy 50.39
2022-02-09 22:02:50,600 [main.py] => Epoch 187/200 => Loss 0.090, Train_accy 98.51, Test_accy 50.95
2022-02-09 22:03:13,129 [main.py] => Epoch 188/200 => Loss 0.082, Train_accy 98.87, Test_accy 50.17
2022-02-09 22:03:35,184 [main.py] => Epoch 189/200 => Loss 0.082, Train_accy 98.84, Test_accy 50.61
2022-02-09 22:03:56,246 [main.py] => Epoch 190/200 => Loss 0.083, Train_accy 98.97, Test_accy 50.95
2022-02-09 22:04:18,638 [main.py] => Epoch 191/200 => Loss 0.083, Train_accy 98.78, Test_accy 50.84
2022-02-09 22:04:40,075 [main.py] => Epoch 192/200 => Loss 0.086, Train_accy 98.57, Test_accy 50.17
2022-02-09 22:05:01,498 [main.py] => Epoch 193/200 => Loss 0.086, Train_accy 98.69, Test_accy 50.84
2022-02-09 22:05:22,559 [main.py] => Epoch 194/200 => Loss 0.084, Train_accy 98.69, Test_accy 50.61
2022-02-09 22:05:43,769 [main.py] => Epoch 195/200 => Loss 0.094, Train_accy 98.48, Test_accy 50.84
2022-02-09 22:06:05,537 [main.py] => Epoch 196/200 => Loss 0.076, Train_accy 99.00, Test_accy 50.72
2022-02-09 22:06:27,347 [main.py] => Epoch 197/200 => Loss 0.085, Train_accy 98.66, Test_accy 50.39
2022-02-09 22:06:49,360 [main.py] => Epoch 198/200 => Loss 0.088, Train_accy 98.60, Test_accy 50.06
2022-02-09 22:07:11,140 [main.py] => Epoch 199/200 => Loss 0.091, Train_accy 98.48, Test_accy 50.61
2022-02-09 22:07:32,037 [main.py] => Epoch 200/200 => Loss 0.078, Train_accy 98.63, Test_accy 50.72
