2022-02-09 21:47:37,832 [main.py] => lr:0.01
 epochs:200
 batch_size:32
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 net_type:resnet18

2022-02-09 21:48:09,757 [main.py] => Epoch 1/200 => Loss 4.958, Train_accy 2.89, Test_accy 4.57
2022-02-09 21:48:24,272 [main.py] => Epoch 2/200 => Loss 4.557, Train_accy 5.51, Test_accy 5.80
2022-02-09 21:48:38,091 [main.py] => lr:0.01
 epochs:200
 batch_size:32
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 net_type:resnet18

2022-02-09 21:49:10,095 [main.py] => Epoch 1/200 => Loss 4.954, Train_accy 2.65, Test_accy 3.79
2022-02-09 21:49:25,419 [main.py] => Epoch 2/200 => Loss 4.569, Train_accy 5.42, Test_accy 5.80
2022-02-09 21:49:39,425 [main.py] => Epoch 3/200 => Loss 4.349, Train_accy 6.75, Test_accy 6.69
2022-02-09 21:49:53,853 [main.py] => Epoch 4/200 => Loss 4.182, Train_accy 9.46, Test_accy 6.80
2022-02-09 21:50:08,229 [main.py] => Epoch 5/200 => Loss 4.031, Train_accy 10.34, Test_accy 11.37
2022-02-09 21:50:22,783 [main.py] => Epoch 6/200 => Loss 3.940, Train_accy 12.08, Test_accy 10.59
2022-02-09 21:50:37,344 [main.py] => Epoch 7/200 => Loss 3.815, Train_accy 12.99, Test_accy 10.70
2022-02-09 21:50:52,187 [main.py] => Epoch 8/200 => Loss 3.702, Train_accy 14.85, Test_accy 12.71
2022-02-09 21:51:06,406 [main.py] => Epoch 9/200 => Loss 3.583, Train_accy 16.34, Test_accy 13.04
2022-02-09 21:51:20,543 [main.py] => Epoch 10/200 => Loss 3.508, Train_accy 17.10, Test_accy 13.49
2022-02-09 21:51:35,033 [main.py] => Epoch 11/200 => Loss 3.404, Train_accy 19.53, Test_accy 14.38
2022-02-09 21:51:49,196 [main.py] => Epoch 12/200 => Loss 3.351, Train_accy 20.05, Test_accy 18.95
2022-02-09 21:52:04,201 [main.py] => Epoch 13/200 => Loss 3.260, Train_accy 20.99, Test_accy 15.38
2022-02-09 21:52:18,844 [main.py] => Epoch 14/200 => Loss 3.186, Train_accy 23.15, Test_accy 17.84
2022-02-09 21:52:32,874 [main.py] => Epoch 15/200 => Loss 3.103, Train_accy 24.19, Test_accy 18.73
2022-02-09 21:52:47,594 [main.py] => Epoch 16/200 => Loss 3.053, Train_accy 25.43, Test_accy 19.06
2022-02-09 21:53:02,190 [main.py] => Epoch 17/200 => Loss 2.904, Train_accy 27.99, Test_accy 20.74
2022-02-09 21:53:17,064 [main.py] => Epoch 18/200 => Loss 2.889, Train_accy 27.17, Test_accy 23.08
2022-02-09 21:53:31,520 [main.py] => Epoch 19/200 => Loss 2.818, Train_accy 30.15, Test_accy 23.75
2022-02-09 21:53:46,085 [main.py] => Epoch 20/200 => Loss 2.683, Train_accy 32.28, Test_accy 17.17
2022-02-09 21:54:00,968 [main.py] => Epoch 21/200 => Loss 2.627, Train_accy 33.19, Test_accy 25.86
2022-02-09 21:54:15,192 [main.py] => Epoch 22/200 => Loss 2.594, Train_accy 33.95, Test_accy 25.98
2022-02-09 21:54:29,055 [main.py] => Epoch 23/200 => Loss 2.487, Train_accy 35.50, Test_accy 24.08
2022-02-09 21:54:43,648 [main.py] => Epoch 24/200 => Loss 2.411, Train_accy 38.24, Test_accy 27.54
2022-02-09 21:54:58,318 [main.py] => Epoch 25/200 => Loss 2.330, Train_accy 39.31, Test_accy 26.09
2022-02-09 21:55:13,158 [main.py] => Epoch 26/200 => Loss 2.308, Train_accy 38.88, Test_accy 26.09
2022-02-09 21:55:27,651 [main.py] => Epoch 27/200 => Loss 2.265, Train_accy 40.04, Test_accy 27.54
2022-02-09 21:55:43,275 [main.py] => Epoch 28/200 => Loss 2.125, Train_accy 43.44, Test_accy 26.31
2022-02-09 21:55:57,245 [main.py] => Epoch 29/200 => Loss 2.100, Train_accy 43.35, Test_accy 31.22
2022-02-09 21:56:11,511 [main.py] => Epoch 30/200 => Loss 1.982, Train_accy 46.97, Test_accy 29.54
2022-02-09 21:56:26,343 [main.py] => Epoch 31/200 => Loss 1.964, Train_accy 46.76, Test_accy 31.88
2022-02-09 21:56:41,174 [main.py] => Epoch 32/200 => Loss 1.819, Train_accy 50.78, Test_accy 29.10
2022-02-09 21:56:55,917 [main.py] => Epoch 33/200 => Loss 1.804, Train_accy 50.75, Test_accy 33.78
2022-02-09 21:57:10,540 [main.py] => Epoch 34/200 => Loss 1.722, Train_accy 53.09, Test_accy 31.55
2022-02-09 21:57:24,850 [main.py] => Epoch 35/200 => Loss 1.680, Train_accy 53.64, Test_accy 32.22
2022-02-09 21:57:40,430 [main.py] => Epoch 36/200 => Loss 1.659, Train_accy 54.61, Test_accy 34.23
2022-02-09 21:57:55,942 [main.py] => Epoch 37/200 => Loss 1.548, Train_accy 57.10, Test_accy 34.45
2022-02-09 21:58:11,038 [main.py] => Epoch 38/200 => Loss 1.488, Train_accy 59.11, Test_accy 34.78
2022-02-09 21:58:26,436 [main.py] => Epoch 39/200 => Loss 1.429, Train_accy 59.84, Test_accy 35.67
2022-02-09 21:58:41,408 [main.py] => Epoch 40/200 => Loss 1.390, Train_accy 62.18, Test_accy 33.00
2022-02-09 21:58:57,220 [main.py] => Epoch 41/200 => Loss 1.372, Train_accy 61.82, Test_accy 35.45
2022-02-09 21:59:13,783 [main.py] => Epoch 42/200 => Loss 1.237, Train_accy 65.29, Test_accy 36.34
2022-02-09 21:59:31,535 [main.py] => Epoch 43/200 => Loss 1.252, Train_accy 64.98, Test_accy 37.35
2022-02-09 21:59:49,336 [main.py] => Epoch 44/200 => Loss 1.179, Train_accy 65.80, Test_accy 36.45
2022-02-09 22:00:07,813 [main.py] => Epoch 45/200 => Loss 1.182, Train_accy 66.08, Test_accy 38.35
2022-02-09 22:00:28,061 [main.py] => Epoch 46/200 => Loss 1.124, Train_accy 68.15, Test_accy 39.35
2022-02-09 22:00:46,504 [main.py] => Epoch 47/200 => Loss 1.091, Train_accy 69.18, Test_accy 40.47
2022-02-09 22:01:05,749 [main.py] => Epoch 48/200 => Loss 1.012, Train_accy 71.22, Test_accy 39.02
2022-02-09 22:01:23,177 [main.py] => Epoch 49/200 => Loss 0.941, Train_accy 72.74, Test_accy 43.03
2022-02-09 22:01:40,420 [main.py] => Epoch 50/200 => Loss 0.963, Train_accy 72.83, Test_accy 40.25
2022-02-09 22:01:57,937 [main.py] => Epoch 51/200 => Loss 0.962, Train_accy 72.35, Test_accy 40.13
2022-02-09 22:02:15,373 [main.py] => Epoch 52/200 => Loss 0.915, Train_accy 74.20, Test_accy 41.03
2022-02-09 22:02:33,116 [main.py] => Epoch 53/200 => Loss 0.877, Train_accy 74.93, Test_accy 41.81
2022-02-09 22:02:50,587 [main.py] => Epoch 54/200 => Loss 0.771, Train_accy 77.97, Test_accy 41.36
2022-02-09 22:03:08,924 [main.py] => Epoch 55/200 => Loss 0.768, Train_accy 77.70, Test_accy 39.69
2022-02-09 22:03:26,546 [main.py] => Epoch 56/200 => Loss 0.764, Train_accy 77.79, Test_accy 45.71
2022-02-09 22:03:44,686 [main.py] => Epoch 57/200 => Loss 0.727, Train_accy 79.37, Test_accy 45.82
2022-02-09 22:04:02,549 [main.py] => Epoch 58/200 => Loss 0.746, Train_accy 79.40, Test_accy 44.48
2022-02-09 22:04:19,678 [main.py] => Epoch 59/200 => Loss 0.676, Train_accy 80.92, Test_accy 44.82
2022-02-09 22:04:36,987 [main.py] => Epoch 60/200 => Loss 0.632, Train_accy 81.05, Test_accy 44.48
2022-02-09 22:04:54,015 [main.py] => Epoch 61/200 => Loss 0.575, Train_accy 83.54, Test_accy 47.38
2022-02-09 22:05:11,325 [main.py] => Epoch 62/200 => Loss 0.575, Train_accy 83.69, Test_accy 47.49
2022-02-09 22:05:28,576 [main.py] => Epoch 63/200 => Loss 0.574, Train_accy 83.48, Test_accy 47.83
2022-02-09 22:05:46,444 [main.py] => Epoch 64/200 => Loss 0.580, Train_accy 82.99, Test_accy 44.82
2022-02-09 22:06:04,218 [main.py] => Epoch 65/200 => Loss 0.569, Train_accy 82.96, Test_accy 49.50
2022-02-09 22:06:21,218 [main.py] => Epoch 66/200 => Loss 0.526, Train_accy 84.88, Test_accy 45.71
2022-02-09 22:06:38,782 [main.py] => Epoch 67/200 => Loss 0.489, Train_accy 86.19, Test_accy 47.27
2022-02-09 22:06:57,005 [main.py] => Epoch 68/200 => Loss 0.468, Train_accy 85.52, Test_accy 47.27
2022-02-09 22:07:14,508 [main.py] => Epoch 69/200 => Loss 0.470, Train_accy 86.49, Test_accy 49.16
2022-02-09 22:07:32,619 [main.py] => Epoch 70/200 => Loss 0.433, Train_accy 87.83, Test_accy 50.84
2022-02-09 22:07:49,098 [main.py] => Epoch 71/200 => Loss 0.449, Train_accy 86.89, Test_accy 48.94
2022-02-09 22:08:05,893 [main.py] => Epoch 72/200 => Loss 0.451, Train_accy 87.04, Test_accy 47.38
2022-02-09 22:08:21,614 [main.py] => Epoch 73/200 => Loss 0.415, Train_accy 87.98, Test_accy 50.84
2022-02-09 22:08:38,307 [main.py] => Epoch 74/200 => Loss 0.370, Train_accy 89.08, Test_accy 46.93
2022-02-09 22:08:54,530 [main.py] => Epoch 75/200 => Loss 0.402, Train_accy 88.83, Test_accy 51.62
2022-02-09 22:09:11,005 [main.py] => Epoch 76/200 => Loss 0.406, Train_accy 88.56, Test_accy 47.94
2022-02-09 22:09:27,514 [main.py] => Epoch 77/200 => Loss 0.419, Train_accy 88.10, Test_accy 50.50
2022-02-09 22:09:44,235 [main.py] => Epoch 78/200 => Loss 0.385, Train_accy 89.11, Test_accy 50.39
2022-02-09 22:10:00,943 [main.py] => Epoch 79/200 => Loss 0.387, Train_accy 88.47, Test_accy 49.05
2022-02-09 22:10:18,047 [main.py] => Epoch 80/200 => Loss 0.333, Train_accy 90.54, Test_accy 50.28
2022-02-09 22:10:33,928 [main.py] => Epoch 81/200 => Loss 0.343, Train_accy 89.60, Test_accy 51.95
2022-02-09 22:10:50,356 [main.py] => Epoch 82/200 => Loss 0.307, Train_accy 91.06, Test_accy 49.28
2022-02-09 22:11:06,085 [main.py] => Epoch 83/200 => Loss 0.306, Train_accy 90.81, Test_accy 51.73
2022-02-09 22:11:22,848 [main.py] => Epoch 84/200 => Loss 0.278, Train_accy 92.27, Test_accy 46.38
2022-02-09 22:11:40,619 [main.py] => Epoch 85/200 => Loss 0.316, Train_accy 91.33, Test_accy 48.83
2022-02-09 22:11:57,345 [main.py] => Epoch 86/200 => Loss 0.259, Train_accy 93.15, Test_accy 50.72
2022-02-09 22:12:14,149 [main.py] => Epoch 87/200 => Loss 0.279, Train_accy 92.15, Test_accy 49.83
2022-02-09 22:12:30,714 [main.py] => Epoch 88/200 => Loss 0.297, Train_accy 91.36, Test_accy 51.73
2022-02-09 22:12:49,038 [main.py] => Epoch 89/200 => Loss 0.292, Train_accy 91.91, Test_accy 51.62
2022-02-09 22:13:11,498 [main.py] => Epoch 90/200 => Loss 0.253, Train_accy 93.12, Test_accy 51.62
2022-02-09 22:13:34,336 [main.py] => Epoch 91/200 => Loss 0.243, Train_accy 92.91, Test_accy 51.17
2022-02-09 22:13:56,356 [main.py] => Epoch 92/200 => Loss 0.222, Train_accy 94.43, Test_accy 52.73
2022-02-09 22:14:15,805 [main.py] => Epoch 93/200 => Loss 0.207, Train_accy 93.92, Test_accy 53.07
2022-02-09 22:14:31,967 [main.py] => Epoch 94/200 => Loss 0.225, Train_accy 93.46, Test_accy 53.51
2022-02-09 22:14:47,927 [main.py] => Epoch 95/200 => Loss 0.245, Train_accy 93.34, Test_accy 52.62
2022-02-09 22:15:04,534 [main.py] => Epoch 96/200 => Loss 0.260, Train_accy 92.55, Test_accy 51.95
2022-02-09 22:15:21,257 [main.py] => Epoch 97/200 => Loss 0.215, Train_accy 94.01, Test_accy 51.73
2022-02-09 22:15:37,909 [main.py] => Epoch 98/200 => Loss 0.212, Train_accy 94.10, Test_accy 50.17
2022-02-09 22:15:54,024 [main.py] => Epoch 99/200 => Loss 0.221, Train_accy 94.01, Test_accy 49.83
2022-02-09 22:16:11,125 [main.py] => Epoch 100/200 => Loss 0.214, Train_accy 93.64, Test_accy 49.94
2022-02-09 22:16:26,792 [main.py] => Epoch 101/200 => Loss 0.145, Train_accy 96.08, Test_accy 55.52
2022-02-09 22:16:42,743 [main.py] => Epoch 102/200 => Loss 0.106, Train_accy 97.35, Test_accy 56.41
2022-02-09 22:16:58,899 [main.py] => Epoch 103/200 => Loss 0.107, Train_accy 97.35, Test_accy 58.31
2022-02-09 22:17:15,611 [main.py] => Epoch 104/200 => Loss 0.090, Train_accy 97.96, Test_accy 57.19
2022-02-09 22:17:32,179 [main.py] => Epoch 105/200 => Loss 0.092, Train_accy 98.05, Test_accy 57.53
2022-02-09 22:17:48,905 [main.py] => Epoch 106/200 => Loss 0.088, Train_accy 97.84, Test_accy 58.31
2022-02-09 22:18:05,248 [main.py] => Epoch 107/200 => Loss 0.077, Train_accy 98.57, Test_accy 58.75
2022-02-09 22:18:21,463 [main.py] => Epoch 108/200 => Loss 0.074, Train_accy 98.30, Test_accy 58.31
2022-02-09 22:18:37,758 [main.py] => Epoch 109/200 => Loss 0.065, Train_accy 98.78, Test_accy 58.64
2022-02-09 22:18:53,915 [main.py] => Epoch 110/200 => Loss 0.075, Train_accy 98.51, Test_accy 58.53
2022-02-09 22:19:15,242 [main.py] => Epoch 111/200 => Loss 0.077, Train_accy 98.11, Test_accy 59.31
2022-02-09 22:19:37,416 [main.py] => Epoch 112/200 => Loss 0.082, Train_accy 98.33, Test_accy 58.42
2022-02-09 22:19:59,217 [main.py] => Epoch 113/200 => Loss 0.068, Train_accy 98.60, Test_accy 58.64
2022-02-09 22:20:21,089 [main.py] => Epoch 114/200 => Loss 0.070, Train_accy 98.33, Test_accy 57.75
2022-02-09 22:20:39,850 [main.py] => Epoch 115/200 => Loss 0.065, Train_accy 98.63, Test_accy 58.53
2022-02-09 22:20:56,816 [main.py] => Epoch 116/200 => Loss 0.059, Train_accy 98.54, Test_accy 58.42
2022-02-09 22:21:13,003 [main.py] => Epoch 117/200 => Loss 0.073, Train_accy 98.14, Test_accy 58.42
2022-02-09 22:21:30,135 [main.py] => Epoch 118/200 => Loss 0.077, Train_accy 98.33, Test_accy 58.75
2022-02-09 22:21:46,465 [main.py] => Epoch 119/200 => Loss 0.068, Train_accy 98.21, Test_accy 58.42
2022-02-09 22:22:02,621 [main.py] => Epoch 120/200 => Loss 0.072, Train_accy 98.39, Test_accy 58.42
2022-02-09 22:22:19,073 [main.py] => Epoch 121/200 => Loss 0.063, Train_accy 98.63, Test_accy 58.31
2022-02-09 22:22:35,571 [main.py] => Epoch 122/200 => Loss 0.062, Train_accy 98.75, Test_accy 58.53
2022-02-09 22:22:52,496 [main.py] => Epoch 123/200 => Loss 0.063, Train_accy 98.51, Test_accy 59.64
2022-02-09 22:23:08,930 [main.py] => Epoch 124/200 => Loss 0.061, Train_accy 98.57, Test_accy 58.64
2022-02-09 22:23:25,838 [main.py] => Epoch 125/200 => Loss 0.057, Train_accy 98.81, Test_accy 58.64
2022-02-09 22:23:42,486 [main.py] => Epoch 126/200 => Loss 0.052, Train_accy 98.97, Test_accy 58.53
2022-02-09 22:23:59,310 [main.py] => Epoch 127/200 => Loss 0.067, Train_accy 98.48, Test_accy 59.09
2022-02-09 22:24:15,577 [main.py] => Epoch 128/200 => Loss 0.065, Train_accy 98.45, Test_accy 57.30
2022-02-09 22:24:32,284 [main.py] => Epoch 129/200 => Loss 0.057, Train_accy 98.75, Test_accy 58.19
2022-02-09 22:24:54,421 [main.py] => Epoch 130/200 => Loss 0.064, Train_accy 98.48, Test_accy 59.31
2022-02-09 22:25:16,817 [main.py] => Epoch 131/200 => Loss 0.058, Train_accy 98.72, Test_accy 59.31
2022-02-09 22:25:39,942 [main.py] => Epoch 132/200 => Loss 0.055, Train_accy 98.84, Test_accy 59.20
2022-02-09 22:26:02,250 [main.py] => Epoch 133/200 => Loss 0.058, Train_accy 98.72, Test_accy 58.42
2022-02-09 22:26:19,119 [main.py] => Epoch 134/200 => Loss 0.065, Train_accy 98.54, Test_accy 59.31
2022-02-09 22:26:34,705 [main.py] => Epoch 135/200 => Loss 0.050, Train_accy 98.90, Test_accy 59.42
2022-02-09 22:26:50,944 [main.py] => Epoch 136/200 => Loss 0.054, Train_accy 98.66, Test_accy 59.87
2022-02-09 22:27:07,743 [main.py] => Epoch 137/200 => Loss 0.051, Train_accy 98.87, Test_accy 57.64
2022-02-09 22:27:23,883 [main.py] => Epoch 138/200 => Loss 0.062, Train_accy 98.51, Test_accy 58.53
2022-02-09 22:27:39,735 [main.py] => Epoch 139/200 => Loss 0.052, Train_accy 98.78, Test_accy 58.64
2022-02-09 22:27:55,970 [main.py] => Epoch 140/200 => Loss 0.049, Train_accy 98.97, Test_accy 58.64
2022-02-09 22:28:12,762 [main.py] => Epoch 141/200 => Loss 0.059, Train_accy 98.69, Test_accy 59.53
2022-02-09 22:28:29,105 [main.py] => Epoch 142/200 => Loss 0.048, Train_accy 99.06, Test_accy 59.64
2022-02-09 22:28:44,941 [main.py] => Epoch 143/200 => Loss 0.057, Train_accy 98.72, Test_accy 58.86
2022-02-09 22:29:01,269 [main.py] => Epoch 144/200 => Loss 0.053, Train_accy 99.00, Test_accy 59.53
2022-02-09 22:29:17,256 [main.py] => Epoch 145/200 => Loss 0.042, Train_accy 99.18, Test_accy 58.53
2022-02-09 22:29:33,673 [main.py] => Epoch 146/200 => Loss 0.056, Train_accy 98.78, Test_accy 59.20
2022-02-09 22:29:49,931 [main.py] => Epoch 147/200 => Loss 0.048, Train_accy 99.09, Test_accy 59.31
2022-02-09 22:30:06,064 [main.py] => Epoch 148/200 => Loss 0.051, Train_accy 99.03, Test_accy 59.87
2022-02-09 22:30:22,863 [main.py] => Epoch 149/200 => Loss 0.052, Train_accy 99.03, Test_accy 59.75
2022-02-09 22:30:38,900 [main.py] => Epoch 150/200 => Loss 0.050, Train_accy 98.78, Test_accy 59.53
2022-02-09 22:30:55,356 [main.py] => Epoch 151/200 => Loss 0.047, Train_accy 98.94, Test_accy 59.09
2022-02-09 22:31:12,016 [main.py] => Epoch 152/200 => Loss 0.045, Train_accy 99.12, Test_accy 58.97
2022-02-09 22:31:28,806 [main.py] => Epoch 153/200 => Loss 0.054, Train_accy 98.87, Test_accy 58.75
2022-02-09 22:31:45,020 [main.py] => Epoch 154/200 => Loss 0.052, Train_accy 99.15, Test_accy 59.87
2022-02-09 22:32:00,899 [main.py] => Epoch 155/200 => Loss 0.057, Train_accy 98.78, Test_accy 60.42
2022-02-09 22:32:17,189 [main.py] => Epoch 156/200 => Loss 0.049, Train_accy 99.06, Test_accy 59.20
2022-02-09 22:32:33,533 [main.py] => Epoch 157/200 => Loss 0.051, Train_accy 98.87, Test_accy 59.53
2022-02-09 22:32:50,044 [main.py] => Epoch 158/200 => Loss 0.049, Train_accy 99.09, Test_accy 58.86
2022-02-09 22:33:06,021 [main.py] => Epoch 159/200 => Loss 0.040, Train_accy 99.21, Test_accy 59.53
2022-02-09 22:33:21,897 [main.py] => Epoch 160/200 => Loss 0.048, Train_accy 99.09, Test_accy 60.09
2022-02-09 22:33:38,169 [main.py] => Epoch 161/200 => Loss 0.052, Train_accy 98.87, Test_accy 58.64
2022-02-09 22:33:55,059 [main.py] => Epoch 162/200 => Loss 0.047, Train_accy 98.84, Test_accy 60.20
2022-02-09 22:34:12,387 [main.py] => Epoch 163/200 => Loss 0.046, Train_accy 99.18, Test_accy 60.09
2022-02-09 22:34:29,558 [main.py] => Epoch 164/200 => Loss 0.054, Train_accy 98.69, Test_accy 59.98
2022-02-09 22:34:46,181 [main.py] => Epoch 165/200 => Loss 0.050, Train_accy 98.94, Test_accy 59.20
2022-02-09 22:35:02,600 [main.py] => Epoch 166/200 => Loss 0.045, Train_accy 99.18, Test_accy 59.42
2022-02-09 22:35:19,165 [main.py] => Epoch 167/200 => Loss 0.051, Train_accy 98.87, Test_accy 59.31
2022-02-09 22:35:36,069 [main.py] => Epoch 168/200 => Loss 0.049, Train_accy 99.00, Test_accy 60.31
2022-02-09 22:35:53,323 [main.py] => Epoch 169/200 => Loss 0.054, Train_accy 98.66, Test_accy 59.98
2022-02-09 22:36:08,836 [main.py] => Epoch 170/200 => Loss 0.048, Train_accy 98.94, Test_accy 60.65
2022-02-09 22:36:24,644 [main.py] => Epoch 171/200 => Loss 0.049, Train_accy 98.97, Test_accy 59.42
2022-02-09 22:36:40,878 [main.py] => Epoch 172/200 => Loss 0.042, Train_accy 99.27, Test_accy 59.87
2022-02-09 22:36:56,702 [main.py] => Epoch 173/200 => Loss 0.054, Train_accy 98.78, Test_accy 58.97
2022-02-09 22:37:13,372 [main.py] => Epoch 174/200 => Loss 0.049, Train_accy 98.75, Test_accy 59.87
2022-02-09 22:37:29,378 [main.py] => Epoch 175/200 => Loss 0.054, Train_accy 98.75, Test_accy 59.64
2022-02-09 22:37:45,043 [main.py] => Epoch 176/200 => Loss 0.055, Train_accy 98.90, Test_accy 60.42
2022-02-09 22:38:00,861 [main.py] => Epoch 177/200 => Loss 0.051, Train_accy 99.00, Test_accy 59.87
2022-02-09 22:38:16,699 [main.py] => Epoch 178/200 => Loss 0.045, Train_accy 99.12, Test_accy 59.87
2022-02-09 22:38:32,661 [main.py] => Epoch 179/200 => Loss 0.049, Train_accy 98.94, Test_accy 59.53
2022-02-09 22:38:48,109 [main.py] => Epoch 180/200 => Loss 0.045, Train_accy 99.00, Test_accy 59.09
2022-02-09 22:39:04,046 [main.py] => Epoch 181/200 => Loss 0.041, Train_accy 99.18, Test_accy 59.42
2022-02-09 22:39:20,312 [main.py] => Epoch 182/200 => Loss 0.048, Train_accy 99.12, Test_accy 59.20
2022-02-09 22:39:36,618 [main.py] => Epoch 183/200 => Loss 0.043, Train_accy 99.39, Test_accy 60.20
2022-02-09 22:39:52,573 [main.py] => Epoch 184/200 => Loss 0.050, Train_accy 98.84, Test_accy 59.64
2022-02-09 22:40:08,016 [main.py] => Epoch 185/200 => Loss 0.046, Train_accy 99.03, Test_accy 59.09
2022-02-09 22:40:24,525 [main.py] => Epoch 186/200 => Loss 0.043, Train_accy 99.18, Test_accy 58.42
2022-02-09 22:40:41,174 [main.py] => Epoch 187/200 => Loss 0.051, Train_accy 98.81, Test_accy 59.31
2022-02-09 22:40:57,615 [main.py] => Epoch 188/200 => Loss 0.046, Train_accy 98.97, Test_accy 58.97
2022-02-09 22:41:14,061 [main.py] => Epoch 189/200 => Loss 0.054, Train_accy 98.81, Test_accy 59.64
2022-02-09 22:41:30,389 [main.py] => Epoch 190/200 => Loss 0.045, Train_accy 99.24, Test_accy 59.09
2022-02-09 22:41:46,807 [main.py] => Epoch 191/200 => Loss 0.048, Train_accy 99.09, Test_accy 59.53
2022-02-09 22:42:02,907 [main.py] => Epoch 192/200 => Loss 0.048, Train_accy 98.97, Test_accy 59.42
2022-02-09 22:42:19,722 [main.py] => Epoch 193/200 => Loss 0.048, Train_accy 99.12, Test_accy 60.65
2022-02-09 22:42:35,962 [main.py] => Epoch 194/200 => Loss 0.045, Train_accy 99.09, Test_accy 59.75
2022-02-09 22:42:52,881 [main.py] => Epoch 195/200 => Loss 0.049, Train_accy 98.94, Test_accy 59.75
2022-02-09 22:43:09,035 [main.py] => Epoch 196/200 => Loss 0.046, Train_accy 98.97, Test_accy 59.31
2022-02-09 22:43:25,029 [main.py] => Epoch 197/200 => Loss 0.045, Train_accy 98.90, Test_accy 59.98
2022-02-09 22:43:40,714 [main.py] => Epoch 198/200 => Loss 0.048, Train_accy 99.06, Test_accy 59.87
2022-02-09 22:43:56,353 [main.py] => Epoch 199/200 => Loss 0.048, Train_accy 99.03, Test_accy 59.20
2022-02-09 22:44:12,660 [main.py] => Epoch 200/200 => Loss 0.050, Train_accy 99.00, Test_accy 58.97
2022-02-11 14:50:06,691 [main.py] => lr:0.01
 epochs:200
 batch_size:32
 optim_type:sgd
 weight_decay:1e-05
 milestones:[100, 150]
 train_root:/data/junjie/Datasets/SD-198
 test_root:/data/junjie/Datasets/SD-198
 net_type:resnet18

2022-02-11 14:50:39,121 [main.py] => Epoch 1/200 => Loss 4.979, Train_accy 3.10, Test_accy 3.01
2022-02-11 14:50:51,618 [main.py] => Epoch 2/200 => Loss 4.588, Train_accy 5.08, Test_accy 6.69
